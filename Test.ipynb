{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Tools\n",
    "import os, sys\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "from pattern.it import parse\n",
    "from pattern.it import pprint\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "\n",
    "\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Visualization\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "from spacy import lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# tokens = nlp(dataset.iloc[0,0])\n",
    "\n",
    "# for token in tokens:\n",
    "#     print(token.text, token.lemma_, stemmer.stem(token.lemma_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lemmas = set()\n",
    "\n",
    "# for doc in nlp.pipe(dataset['text'], disable=[\"tagger\", \"ner\", \"parser\", \"tagger\", \"textcat\"]):\n",
    "#     for token in doc:\n",
    "#         lemmas.add(token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas = []\n",
    "# for index, document in dataset.iterrows():\n",
    "#     tokens = nlp(row['text'])\n",
    "#     for doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\"]):\n",
    "#     for token in tokens:\n",
    "#         if token.text not in string.punctuation and len(token.text) > 3 and len(token.text) < 16:\n",
    "#             print(token.text)\n",
    "#             lemmas.append(token.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28754"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = \"/Users/lpdef/Desktop/Polito/Data Science lab process and methods/Final Exam/dataset_winter_2020/\"\n",
    "dataset = pd.read_csv(directory + \"development.csv\")\n",
    "testset = pd.read_csv(directory + \"evaluation.csv\")\n",
    "\n",
    "corpus = pd.concat([dataset.iloc[:,0],testset.iloc[:,0]])\n",
    "\n",
    "dataset.head()\n",
    "dataset.iloc[:,0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset.iloc[:,0]\n",
    "# doc = nlp(dataset.iloc[0,0])\n",
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spacy_stopWords = spacy.lang.it.stop_words.STOP_WORDS\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lemmatizer = spacy.load(\"it_core_news_sm\", disable=[\"ner\", \"parser\", \"tagger\", \"textcat\"])\n",
    "        self.Porter_Stemmer = SnowballStemmer(\"italian\")\n",
    "        \n",
    "        \n",
    "    def __call__(self, document):\n",
    "#         print(document)\n",
    "        lemmas = []\n",
    "        re_digit = re.compile(\"[0-9]\")\n",
    "        document = document.replace(\"'\", \" \")\n",
    "\n",
    "        for t in self.lemmatizer(document):\n",
    "            if bool(re.match(\"[0-9]+\", t.lemma_))== False:\n",
    "                t = t.lemma_.strip()\n",
    "                t = t.translate(str.maketrans('', '', string.punctuation))\n",
    "                \n",
    "                if bool(t.isalpha())==True:\n",
    "                    if len(t) > 1 and len(t) < 20:\n",
    "                        t = self.Porter_Stemmer.stem(t)\n",
    "                        lemmas.append(t)\n",
    "\n",
    "        \n",
    "            \n",
    "        return lemmas\n",
    "    \n",
    "\n",
    "stopWords = sw.words('italian')\n",
    "stopWords.remove('ma')\n",
    "stopWords.remove('con')\n",
    "stopWords.extend(['avra', 'avro', 'sar', 'aver', 'com', 'contr', 'dar', 'esser', 'far', 'fec', 'foss', \n",
    "                  'lor', 'nostr', 'perc', 'qual', 'quant', 'quell', 'quest', 'star', 'stemm', \n",
    "                  'stess', 'stetter', 'tutt', 'vostr',\n",
    "                 \n",
    "                 'dio', 'neo', 'piu',\n",
    "                  \n",
    "                 'luned', 'marted', 'mercoled', 'gioved', 'venerd', 'sabat', 'domenic',\n",
    "                  \n",
    "                 'aglo', 'avere', 'avutare', 'avutere', 'dallare', 'ebbo', 'essere',\n",
    "                 'facciata', 'facere', 'fara', 'faro', 'farsene', 'fossa', 'fosso', 'mieo', 'perca',\n",
    "                 'sara', 'saro', 'stara', 'stare', 'stessa', 'staro', 'stesso', 'suoo', 'tuoo', 'aver',\n",
    "                 'avut', 'fac', 'facc', 'farse', 'nostr'\n",
    "                 \n",
    "                 'fars', 'farsen'])\n",
    "                 \n",
    "#                  'gennai', 'febbrai', 'marz', 'april', 'magg', 'giungn', 'lugl', 'agost', 'settembr', 'ottobr', 'novembr', 'dicembr',\n",
    "#                  'lorenz', 'anna', 'marc', 'francesc',\n",
    "\n",
    "#                 'molt, '])\n",
    "\n",
    "\n",
    "# stopWords.extend(spacy_stopWords)\n",
    "\n",
    "lemmaTokenizer = LemmaTokenizer()\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmaTokenizer, stop_words=stopWords, max_df=0.8, min_df=0.0005, \n",
    "                             strip_accents='unicode', lowercase=True, ngram_range=(1,3), sublinear_tf=True)\n",
    "\n",
    "\n",
    "# X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lpdef/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['fars'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.8, max_features=None,\n",
       "                min_df=0.0005, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True,\n",
       "                stop_words=['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl',\n",
       "                            'alla', 'alle', 'col', 'coi', 'da', 'dal', 'dallo',\n",
       "                            'dai', 'dagli', 'dall', 'dagl', 'dalla', 'dalle',\n",
       "                            'di', 'del', 'dello', 'dei', 'degli', 'dell',\n",
       "                            'degl', 'della', 'delle', 'in', ...],\n",
       "                strip_accents='unicode', sublinear_tf=True,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<__main__.LemmaTokenizer object at 0x1a3b4d3910>,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lista= [dataset.iloc[2300,0],]\n",
    "# vectorizer.fit(np.array(lista))\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = vectorizer.transform(testset.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"mal-\" in vectorizer.get_feature_names())\n",
    "# print(\"pess-\" in vectorizer.get_feature_names())\n",
    "# print(\"malissim\" in vectorizer.get_feature_names())\n",
    "# print(\"problem-\" in vectorizer.get_feature_names())\n",
    "# print(\"buon-\" in vectorizer.get_feature_names())\n",
    "# print(\"sconsigl-\" in vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21496"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# # Given matrix X and min_npoints, plot K-Neighbors chart\n",
    "# # It also plots a horizontal line in correspondence to eps value\n",
    "# def plot_neighbors(X, min_points, eps=0):\n",
    "#     # \"min_points + 1\" because se first column in \"distances\" is the distance of each point and itself\n",
    "#     nbrs = NearestNeighbors(n_neighbors=min_points + 1, n_jobs = -1).fit(X)\n",
    "#     distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "#     k_dist = distances[:, -1]\n",
    "#     k_dist.sort()\n",
    "#     x_axis = np.arange(k_dist.shape[0])\n",
    "\n",
    "#     ### Disable comment below to activate interactive plots\n",
    "#     #%matplotlib notebook\n",
    "\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.plot(x_axis, k_dist, linestyle='', marker='o', markersize=1)\n",
    "    \n",
    "#     # Plot a horizontal line in correspondence to eps value\n",
    "#     ax.hlines(eps, x_axis.min(), x_axis.max(), linestyle='--')\n",
    "#     ax.set_title(f\"K-Neighbors chart. Min_points = {min_points}, eps={eps}.\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot charts\n",
    "# plot_neighbors(X_train, 300, eps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopWords.remove('ma')\n",
    "# # stopWords.remove('con')\n",
    "# 'ma' in stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run clustering\n",
    "# model_db10 = DBSCAN(eps = 1.30, min_samples=300, n_jobs = -1)\n",
    "# # model_db20 = DBSCAN(eps = 0.1, min_samples=20)\n",
    "# res_db10 = model_db10.fit_predict(X_train)\n",
    "# # res_db20 = model_db20.fit_predict(X_noise)\n",
    "\n",
    "# # Show results\n",
    "# # %matplotlib inline\n",
    "# # fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# # ax[0].set_title(\"DBSCAN, minpoints 10\")\n",
    "# # # ax[1].set_title(\"DBSCAN, minpoints 20\")\n",
    "# # ax[0].scatter(X_train[:,0], X_train[:,1], s=10, c=res_db10)\n",
    "# # # ax[1].scatter(X_noise[:,0], X_noise[:,1], s=10, c=res_db20)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_db10.labels_[model_db10.labels_==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# l = set()\n",
    "\n",
    "# for doc in nlp.pipe(vectorizer.get_feature_names(), disable=[\"tagger\", \"ner\", \"parser\", \"textcat\"]):\n",
    "#     for token in doc:\n",
    "#         print(token.text, token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = nlp(\"abbandonato\")\n",
    "# for token in tokens:\n",
    "#     print(token.text, token.lemma_)\n",
    "    \n",
    "    \n",
    "# for doc in nlp.pipe([\"abbandonato\",], disable=[\"tagger\", \"ner\", \"parser\", \"textcat\"]):\n",
    "#     for token in doc:\n",
    "#         print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# param_grid = {'C':list(range(2,10,2)), 'penalty':['l2'], 'solver':['liblinear']}\n",
    "# y_train = dataset.iloc[:, 1]\n",
    "\n",
    "# clf = LogisticRegression(verbose=1, random_state=0, max_iter=10000)\n",
    "\n",
    "# gridSearch = GridSearchCV(clf, param_grid, scoring='f1_weighted', cv=5)\n",
    "\n",
    "# gridSearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method = TSNE(n_components=2, random_state=0)\n",
    "# Y = method.fit_transform(red_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL_COLOR_MAP = {'pos' : 'r',\n",
    "#                    'neg' : 'k'}\n",
    "# label_color = [LABEL_COLOR_MAP[l] for l in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# ax.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral, c=label_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Macro-f1 for each iteration: [0.96653241 0.96979185 0.96001167 0.96212441 0.96348172]\n",
      "Macro-f1 (statistics): 0.964 (+/- 0.007)\n"
     ]
    }
   ],
   "source": [
    "y_train = dataset.iloc[:, 1]\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "clf = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=6, penalty='l2',max_iter=1000)\n",
    "# clf = SVC()\n",
    "# clf = SVC(kernel='sigmoid', gamma=1.0, degree=15, random_state=0)\n",
    "# clf = SVC(kernel='sigmoid', gamma=1.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pipeline = make_pipeline(MaxAbsScaler(), clf)\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "f1_cv = cross_val_score(clf, X_train, y_train, cv=5, scoring='f1_weighted')\n",
    "\n",
    "print(f\"Macro-f1 for each iteration: {f1_cv}\")\n",
    "mean_macro_f1 = f1_cv.mean()\n",
    "std_macro_f1 = f1_cv.std() * 2\n",
    "print(f\"Macro-f1 (statistics): {mean_macro_f1:.3f} (+/- {std_macro_f1:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]\n",
      "[[ 9101   121]\n",
      " [   63 19469]]\n",
      "24 pos neg\n",
      "42 neg pos\n",
      "131 neg pos\n",
      "337 pos neg\n",
      "593 pos neg\n",
      "700 neg pos\n",
      "749 neg pos\n",
      "806 neg pos\n",
      "826 pos neg\n",
      "1062 neg pos\n",
      "1149 pos neg\n",
      "1410 pos neg\n",
      "1526 pos neg\n",
      "1614 neg pos\n",
      "1695 neg pos\n",
      "1898 pos neg\n",
      "2090 neg pos\n",
      "2096 pos neg\n",
      "2393 neg pos\n",
      "2562 neg pos\n",
      "2844 neg pos\n",
      "2851 neg pos\n",
      "2893 pos neg\n",
      "3134 neg pos\n",
      "3467 neg pos\n",
      "3469 pos neg\n",
      "3502 neg pos\n",
      "3525 neg pos\n",
      "3618 pos neg\n",
      "3779 pos neg\n",
      "3998 neg pos\n",
      "4652 neg pos\n",
      "4715 pos neg\n",
      "4740 pos neg\n",
      "5209 pos neg\n",
      "5419 neg pos\n",
      "5481 neg pos\n",
      "5569 pos neg\n",
      "5590 neg pos\n",
      "5598 pos neg\n",
      "6081 neg pos\n",
      "6547 neg pos\n",
      "6690 pos neg\n",
      "6785 neg pos\n",
      "6824 neg pos\n",
      "6990 neg pos\n",
      "7058 neg pos\n",
      "7087 neg pos\n",
      "7123 neg pos\n",
      "7303 neg pos\n",
      "7340 neg pos\n",
      "7554 neg pos\n",
      "8679 pos neg\n",
      "9207 neg pos\n",
      "9291 neg pos\n",
      "9849 neg pos\n",
      "10555 pos neg\n",
      "10575 neg pos\n",
      "10672 neg pos\n",
      "10849 neg pos\n",
      "10860 neg pos\n",
      "10881 neg pos\n",
      "10946 neg pos\n",
      "11083 neg pos\n",
      "11140 neg pos\n",
      "11717 pos neg\n",
      "11774 pos neg\n",
      "11801 neg pos\n",
      "12122 neg pos\n",
      "12144 neg pos\n",
      "12147 pos neg\n",
      "12388 neg pos\n",
      "12548 neg pos\n",
      "12620 neg pos\n",
      "12710 neg pos\n",
      "12767 neg pos\n",
      "12797 neg pos\n",
      "12856 neg pos\n",
      "12899 pos neg\n",
      "12967 neg pos\n",
      "13004 neg pos\n",
      "13076 neg pos\n",
      "13414 neg pos\n",
      "13426 neg pos\n",
      "13448 neg pos\n",
      "13475 pos neg\n",
      "13566 neg pos\n",
      "13603 pos neg\n",
      "13752 neg pos\n",
      "13777 neg pos\n",
      "13904 neg pos\n",
      "13973 pos neg\n",
      "14074 pos neg\n",
      "14198 neg pos\n",
      "14332 neg pos\n",
      "14458 pos neg\n",
      "14482 pos neg\n",
      "14521 neg pos\n",
      "14536 neg pos\n",
      "14615 neg pos\n",
      "14691 neg pos\n",
      "15166 pos neg\n",
      "15187 neg pos\n",
      "15476 neg pos\n",
      "15693 neg pos\n",
      "16003 pos neg\n",
      "16790 pos neg\n",
      "16913 pos neg\n",
      "17244 neg pos\n",
      "17252 neg pos\n",
      "17450 pos neg\n",
      "17609 pos neg\n",
      "17672 pos neg\n",
      "17991 neg pos\n",
      "18266 pos neg\n",
      "18320 neg pos\n",
      "18363 pos neg\n",
      "18700 neg pos\n",
      "18753 neg pos\n",
      "18962 pos neg\n",
      "19039 neg pos\n",
      "19198 neg pos\n",
      "19302 pos neg\n",
      "19357 pos neg\n",
      "19810 pos neg\n",
      "19921 neg pos\n",
      "19998 neg pos\n",
      "20057 pos neg\n",
      "20378 pos neg\n",
      "20700 pos neg\n",
      "20886 neg pos\n",
      "21006 pos neg\n",
      "21061 neg pos\n",
      "21260 neg pos\n",
      "21303 neg pos\n",
      "21367 pos neg\n",
      "21477 neg pos\n",
      "21489 pos neg\n",
      "21501 pos neg\n",
      "21528 neg pos\n",
      "21722 pos neg\n",
      "21786 neg pos\n",
      "21926 neg pos\n",
      "21995 neg pos\n",
      "22308 neg pos\n",
      "22391 neg pos\n",
      "22404 neg pos\n",
      "22455 neg pos\n",
      "22714 neg pos\n",
      "22722 neg pos\n",
      "23116 neg pos\n",
      "23452 pos neg\n",
      "23731 neg pos\n",
      "23926 neg pos\n",
      "24001 neg pos\n",
      "24067 neg pos\n",
      "24084 neg pos\n",
      "24188 neg pos\n",
      "24270 neg pos\n",
      "24632 neg pos\n",
      "25185 neg pos\n",
      "25732 pos neg\n",
      "26042 neg pos\n",
      "26056 neg pos\n",
      "26268 neg pos\n",
      "26321 neg pos\n",
      "26520 pos neg\n",
      "26546 pos neg\n",
      "26597 neg pos\n",
      "26823 pos neg\n",
      "26988 neg pos\n",
      "27678 pos neg\n",
      "27755 pos neg\n",
      "27813 pos neg\n",
      "27947 neg pos\n",
      "28246 neg pos\n",
      "28473 neg pos\n",
      "28527 neg pos\n",
      "28539 pos neg\n",
      "28582 neg pos\n",
      "28600 neg pos\n",
      "28629 pos neg\n",
      "28684 neg pos\n",
      "28714 pos neg\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "print()\n",
    "conf_mat = confusion_matrix(y_train, y_pred)\n",
    "print(conf_mat)\n",
    "count = 0;\n",
    "for i, label in enumerate(y_pred):\n",
    "    if y_train[i] is not label:\n",
    "        count = count + 1\n",
    "        print(i,y_train[i],label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tipo di tasso difficile.\\nIl punto è: se si considera ciò che si paga e quello che si ottiene, si cerca un raffinato hotel a quattro stelle.\\nSe vi aspettate un esperienza a cinque stelle al prezzo di un 4 stelle, allora resterete delusi.\\n\\nNon voglio entrare nei dettagli. Appena livello le vostre aspettative di un four-star facility e non ve ne pentirete avendo scelto Golden Palace.\\nSolo una raccomandazione: dato che non c'è servizio di lavanderia all'interno dei locali (se ho azzeccato, usano un nelle vicinanze Independent business che, come ci si potrebbe aspettare, chiude a un certo punto nel pomeriggio e durante il fine settimana), e nemmeno le camere sono dotate di un ferro da stiro, è meglio avere una camicia stirata correttamente nel vostro bagaglio se arrivate all'hotel dall'aeroporto e sono state pianificando di andare direttamente a cena fuori dopo la doccia!\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[1149,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12323, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_to_file(filename, labels):\n",
    "    \"\"\"Dump the evaluated labels to a CSV file.\"\"\"\n",
    "    with open(filename, mode='w', encoding='UTF-8') as f:\n",
    "        fwriter = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        fwriter.writerow([\"Id\", \"Predicted\"])\n",
    "        for Id, label in enumerate(labels):\n",
    "            fwriter.writerow([Id, label])\n",
    "            \n",
    "\n",
    "dump_to_file(\"Labels.csv\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il nostro primo soggiorno a Napoli e abbiamo scelto questo hotel dal British Airways\" consigliato elenco - e siamo contenti così. Ottima vista sulla Baia verso Capri e Castell dell'Ovo con molti servizi locali - soprattutto ristoranti, ma anche ottimi collegamenti bus route, una vicina fermata del taxi e un traffico - gratis a piedi la città lungo il fronte mare o sulla collina lungo la Via Chiaia.\n",
      "L'hotel stesso offre un'ottima colazione e camere eleganti, ma consigliamo pagare di più per la vista sul mare.\n",
      "\n",
      "pos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(testset.iloc[1000,0])\n",
    "print()\n",
    "print(y_pred[1000])\n",
    "\n",
    "y_pred.shape[0] == testset.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'stara' in vectorizer.get_feature_names()\n",
    "\n",
    "# vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = nlp(\"andavo\", disable=[\"tagger\", \"ner\", \"parser\", \"tagger\", \"textcat\"])\n",
    "# for token in tokens:\n",
    "#     print(token.lemma_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
